---
title: "EDS 231 - Sentiment Analysis II - Twitter"
author: "Juliet Cohen"
date: '2022-04-26'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(quanteda)
library(quanteda.sentiment)
library(quanteda.textstats)
library(tidyverse)
library(tidytext)
library(lubridate)
library(wordcloud) 
library(reshape2)
library(qdapRegex)
library(kableExtra)
library(sentimentr)
library(patchwork)
```

Read in Twitter data:
```{r}
raw_tweets <- read.csv("/Users/juliet/Documents/MEDS/Text_Analysis/Text_Analysis/Assignment_3/IPCC_tweets_April1-10_sample.csv", header=TRUE)

# Extract Date and Title fields
data <- raw_tweets[,c(4,6)] 

# create a tibble from the tweet title and date columns
tweets <- tibble(text = data$Title,
                  id = seq(1:length(data$Title)),
                 date = as.Date(data$Date,'%m/%d/%y'))

# simple plot of tweets per day
tweets %>%
  count(date) %>%
  ggplot(aes(x = date, y = n))+
  geom_line() +
  labs(title = "Number of Tweets per day")
```

# Data Cleaning

## Think about how to further clean a twitter data set. Let’s assume that the mentions of twitter accounts is not useful to us. Remove them from the text field of the tweets tibble.

To clean the data, I removed website links, emojis, @ symbols, and # symbols because mentions of twitter accounts are not useful to us, plus I converted all text to lower case and removed "'s", "'t", and digits.

```{r}
# let's clean up the URLs from the tweets
tweets$text <- gsub("http[^[:space:]]*", "", tweets$text)

# remove emojis
tweets$text <- iconv(tweets$text, "latin1", "ASCII", sub="")

# remove @ and the name of the account tagged because we dont need tagged people in this analysis, just the seniment words
tweets$text <- gsub("@[^[:space:]]*", "", tweets$text)

# remove # but keep the word following
tweets$text <- gsub("#", "", tweets$text)

# remove the @ symbol and the account that follows
tweets$text <- gsub("@[a-z,A-Z]*", "", tweets$text)

# convert all text to lower case
tweets$text <- str_to_lower(tweets$text)

# remove 's 
tweets$text <- gsub("'s", "", tweets$text)

# remove 't 
tweets$text <- gsub("'t", "", tweets$text)

# remove digits
tweets$text <- str_remove_all(tweets$text, "[:digit:]")

#load sentiment lexicons as usual
bing_sent <- get_sentiments('bing')
nrc_sent <- get_sentiments('nrc')

# tokenize tweets to individual words so they will be 1 word per row
words <- tweets %>%
  select(id, date, text) %>%
  unnest_tokens(output = word, input = text, token = "words") %>%
  anti_join(stop_words, by = "word") %>% # remove stop words
  left_join(bing_sent, by = "word") %>% # join the words to the sentiment words (label with a sentiment)
  left_join(
    tribble(
      ~sentiment, ~sent_score,
      "positive", 1,
      "negative", -1),
    by = "sentiment")

# the new sentiment score column is numerical, it is how we assign sentiment to words
# besides just pos/neg/neutral/etc.
```

## Compare the ten most common terms in the tweets per day. Do you notice anything interesting?

I noticed that there were much higher word counts on the dates on April 4th, followed by April 5th and 6th, and the word "ipcc"  and "climate" appear on all days. I also noticed that the word "fossil" appears as a top word on many of the days. Overall, the words are more practical, scientific words rather than words that are strongly associated with emotions.

By looking at the total top 10 words over all these days, we can see that they are all reflective of what I would expect to see for this topic.

```{r}
# examine trends of the top 10 words per day
top_daily_words <- words %>% 
  group_by(date, word) %>% 
  summarize(count = n()) %>% 
  arrange(desc(count)) %>%
  slice(1:10)

# look for the individual words "ipcc", "fossil", "time", and "climate"
ipcc_mentions <- top_daily_words %>% 
  filter(word == "ipcc")

fossil_mentions <- top_daily_words %>% 
  filter(word == "fossil")

time_mentions <- top_daily_words %>% 
  filter(word == "time")

climate_mentions <- top_daily_words %>% 
  filter(word == "climate")

top_daily_words_table <- kable(top_daily_words, 
                         caption = "Top 10 Daily Words by Day")
top_daily_words_table

# examine trends of the top 10 words total
top_10_total <- words %>%
  group_by(word) %>% 
  summarize(count = n()) %>% 
  slice_max(count, n = 10) 

top_10_total_table <- kable(top_10_total, 
                         caption = "Top 10 Daily Words Total")
top_10_total_table

# plot the top 10 words total 
ggplot(data = top_10_total, aes(y = reorder(word, count), x = count)) +
  geom_col(aes(fill = word, width = .5)) +
  theme_classic() +
  labs(title = "Top 10 Words Total\nApril 1st, 2022 - April 10th, 2022",
       x = "Count",
       y = "Word")
```

## Adjust the wordcloud in the “wordcloud” chunk by coloring the positive and negative words so they are identifiable.

```{r}
# sentiment wordcloud
words %>%
  # attach sentiments
inner_join(get_sentiments("bing")) %>%
  # count the sentiment words, showing largest groups at the top 
count(word, sentiment, sort = TRUE) %>%
  # aggregate data by sentiment 
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("firebrick", "forestgreen"),
                 max.words = 100,
                 title.colors = c("firebrick", "forestgreen"))
```

## Let’s say we are interested in the most prominent entities in the Twitter discussion. Which are the top 10 most tagged accounts in the data set. Hint: the “explore_hashtags” chunk is a good starting point.

```{r corpus_&_quantdata_cleaning}
# import the corpus for this data
corpus <- corpus(data$Title)
#summary(corpus)

# make the corpus into a tokens object, without punctuation, do not remove numbers because they might be part of a tagged account name
tokens <- tokens(corpus, remove_punct = TRUE)

# only keep those that have tagged accounts, then convert that into a document-feature matrix and rename the col for clarity
tags <- tokens %>% 
  tokens_keep(pattern = "@*") %>% 
  dfm() %>% 
  textstat_frequency(n = 10) %>% 
  rename(tag = feature)

ggplot(data = tags, aes(x = frequency, y = reorder(tag, frequency))) +
  geom_col(aes(fill = tag)) +
  theme_classic() +
  labs(x = "Count",
       y = "Tagged Accounts",
       title = "Top 10 Most Tagged Accounts\nApril 1st, 2022 - April 10th, 2022")
```

## The Twitter data download comes with a variable called “Sentiment” that must be calculated by Brandwatch. Use your own method to assign each tweet a polarity score (Positive, Negative, Neutral) and compare your classification to Brandwatch’s (hint: you’ll need to revisit the “raw_tweets” data frame).

```{r}
# tokenize tweets to individual words so they will be 1 word per row
sentiment <- tweets %>%
  select(id, date, text) %>%
  unnest_tokens(output = word, input = text, token = "words") %>%
  anti_join(stop_words, by = "word") %>% # remove stop words
  left_join(bing_sent, by = "word") %>% # join the words to the sentiment words (label with a sentiment)
  left_join(
    tribble(
      ~sentiment, ~sent_score,
      "positive", 1,
      "negative", -1),
    by = "sentiment")

my_sentiment <- get_sentences(data$Title) %>% 
  sentiment() %>% 
  group_by(element_id) %>% 
  summarize(sentiment_score = mean(sentiment)) %>% 
  mutate(sentiment = case_when(
    sentiment_score < 0 ~ "negative",
    sentiment_score == 0 ~ "neutral",
    sentiment_score > 0 ~ "positive")) %>% 
  group_by(sentiment) %>% 
  summarize(count = n()) %>% 
  mutate(color = c("firebrick", "purple", "forestgreen"))

my_plot <- ggplot(data = my_sentiment, aes(x = count, y = reorder(sentiment, count), fill = color)) + 
  geom_col(stat="identity") +
  xlim(0, 2400) +
  scale_fill_identity() +
  labs(x = "",
       y = "",
       subtitle = "My Sentiment Analysis")

# Brandwatch sentiment 
raw_tweets <- read.csv("/Users/juliet/Documents/MEDS/Text_Analysis/Text_Analysis/Assignment_3/IPCC_tweets_April1-10_sample.csv", header=TRUE)

# Extract Date and Title fields
data <- raw_tweets[,c(4,6, 10:11)] 

clean_tweets <- tibble(id = seq(1:length(data$Title)),
                 text = data$Title,
                 date = as.Date(data$Date,'%m/%d/%y'),
                 sentiment = data$Sentiment,
                 emotion = data$Emotion)

brandwatch_sentiment <- clean_tweets %>%
  filter(sentiment %in% c("positive", "negative", "neutral")) %>% 
  group_by(sentiment) %>% 
  summarize(count = n()) %>% 
  mutate(color = c("firebrick", "purple", "forestgreen"))

brandwatch_plot <- ggplot(data = brandwatch_sentiment, aes(x = count, y = reorder(sentiment, count), fill = color)) +
  geom_col() +
  xlim(0, 2400) +
  scale_fill_identity() +
  labs(x = "",
       y = "",
       subtitle = "Brandwatch Sentiment Analysis")

# use patchwork to combine plots
combined_plots <- (brandwatch_plot / my_plot) +
  plot_annotation(title = "Tweet Sentiment: My Analysis versus Brandwatch Analysis") +
  plot_layout(guides = "collect")

combined_plots
```




