---
title: "EDS 231 - Sentiment_Analysis_II"
author: "Juliet Cohen"
date: '2022-04-26'
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(quanteda)
library(quanteda.sentiment)
library(quanteda.textstats)
library(tidyverse)
library(tidytext)
library(lubridate)
library(wordcloud) 
library(reshape2)
library(qdapRegex)
library(kableExtra)
```

Read in Twitter data:
```{r}
raw_tweets <- read.csv("/Users/juliet/Documents/MEDS/Text_Analysis/Text_Analysis/Assignment_3/IPCC_tweets_April1-10_sample.csv", header=TRUE)

# Extract Date and Title fields
data <- raw_tweets[,c(4,6)] 

# create a tibble from the tweet title and date columns
tweets <- tibble(text = data$Title,
                  id = seq(1:length(data$Title)),
                 date = as.Date(data$Date,'%m/%d/%y'))

# head(tweets$text, n = 10)

# simple plot of tweets per day
tweets %>%
  count(date) %>%
  ggplot(aes(x = date, y = n))+
  geom_line() +
  labs(title = "Number of Tweets per day")
```

# Data Cleaning

Think about how to further clean a twitter data set. Let’s assume that the mentions of twitter accounts is not useful to us. Remove them from the text field of the tweets tibble.

```{r}
# let's clean up the URLs from the tweets
tweets$text <- gsub("http[^[:space:]]*", "", tweets$text)

# remove emojis
tweets$text <- iconv(tweets$text, "latin1", "ASCII", sub="")

# remove @ and the name of the account tagged because we dont need tagged people in this analysis, just the seniment words
tweets$text <- gsub("@[^[:space:]]*", "", tweets$text)

# remove # but keep the word following
tweets$text <- gsub("#", "", tweets$text)

# convert all text to lower case
tweets$text <- str_to_lower(tweets$text)

# remove quotes
#tweets$text <- gsub("'", "", tweets$text)

#load sentiment lexicons as usual
bing_sent <- get_sentiments('bing')
nrc_sent <- get_sentiments('nrc')

# tokenize tweets to individual words so they will be 1 word per row
words <- tweets %>%
  select(id, date, text) %>%
  unnest_tokens(output = word, input = text, token = "words") %>%
  anti_join(stop_words, by = "word") %>% # remove stop words
  left_join(bing_sent, by = "word") %>% # join the words to the sentiment words (label with a sentiment)
  left_join(
    tribble(
      ~sentiment, ~sent_score,
      "positive", 1,
      "negative", -1),
    by = "sentiment")
# the new sentiment score column is numerical, it is how we assign sentiment to words besides just pos/neg
```

Compare the ten most common terms in the tweets per day. Do you notice anything interesting?

```{r}
# examine trends of the top 10 words per day
top_daily_words <- words %>% 
  group_by(date, word) %>% 
  summarize(count = n()) %>% 
  arrange(desc(count)) %>%
  slice(1:10)

top_daily_words_table <- kable(top_daily_words, 
                         caption = "Top 10 Daily Words by Day")
top_daily_words_table

# examine trends of the top 10 words total
top_10_total <- top_daily_words %>%
  ungroup() %>% 
  slice_max(count, n = 10)

top_10_words_total_table <- kable(top_10_total, 
                         caption = "Top 10 Daily Words Total")
top_10_words_total_table

#unique_word_counts <- unique(sort(daily_word_counts$count, decreasing = TRUE))
```

Adjust the wordcloud in the “wordcloud” chunk by coloring the positive and negative words so they are identifiable.

```{r}
# most common words in general
words %>%
   anti_join(stop_words) %>%
   count(word) %>%
   with(wordcloud(word, n, max.words = 100))

# sentiment wordcloud
words %>%
  # attach sentiments
inner_join(get_sentiments("bing")) %>%
  # count the sentiment words, showing largest groups at the top 
count(word, sentiment, sort = TRUE) %>%
  # aggregate data by sentiment 
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("firebrick", "forestgreen"),
                   max.words = 100)
```


Let’s say we are interested in the most prominent entities in the Twitter discussion. Which are the top 10 most tagged accounts in the data set. Hint: the “explore_hashtags” chunk is a good starting point.

```{r}
corpus <- corpus(data$Title)
#summary(corpus)

# initial cleaning
tokens <- tokens(corpus) # tokenize the text so each doc (page, in this case) is a list of tokens (words), the words are bound by their presence in the same text ID, so they are lists

# examine the uncleaned version
tokens

# clean it up, we dont want commas or the word "a"
tokens <- tokens(tokens, remove_punct = TRUE,
                      remove_numbers = TRUE) # sometimes you might want to keep numbers, in this case we remove them, but we only remove stand alone numbers, not numbers that are linked to words

tokens <- tokens_select(tokens, stopwords('english'),selection='remove') #stopwords lexicon built in to quanteda

tokens <- tokens_tolower(tokens)
```

```{r}
tag_tweets <- tokens(corpus, remove_punct = TRUE) %>% 
               tokens_keep(pattern = "@*")

# determine which order the words occurred in the tweets
dfm_tag<- dfm(tag_tweets)

tag_freq <- textstat_frequency(dfm_tag, n = 100)

# docfreq = amount of docs (tweets) the #word occurred in 
head(tag_freq, 10)

# tidytext gives us tools to convert to tidy from non-tidy formats
tag_tib<- tidy(dfm_tag) # tidy() works for document frequency matrices

tag_tib %>%
   count(term) %>%
   with(wordcloud(term, n, max.words = 100))
```


The Twitter data download comes with a variable called “Sentiment” that must be calculated by Brandwatch. Use your own method to assign each tweet a polarity score (Positive, Negative, Neutral) and compare your classification to Brandwatch’s (hint: you’ll need to revisit the “raw_tweets” data frame).

```{r}
raw_tweets
```




