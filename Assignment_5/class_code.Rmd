---
title: "Topic 6: Topic Analysis"
author: "Juliet Cohen"
date: '2022-05-09'
output: pdf_document
---

```{r packages}
library(here)
library(pdftools)
library(quanteda)
library(tm)
library(topicmodels)
library(ldatuning)
library(tidyverse)
library(tidytext)
library(reshape2)
```

Load the data

```{r data}
##Topic 6 .Rmd here:https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/topic_6.Rmd
#grab data here: 
comments_df<-read_csv("https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/dat/comments_df.csv")

#comments_df <- read_csv(here("dat", "comments_df.csv")) #if reading from local
```

Now we'll build and clean the corpus

```{r corpus}
epa_corp <- corpus(x = comments_df, text_field = "text")
epa_corp.stats <- summary(epa_corp)
head(epa_corp.stats, n = 25)

# create tokens obj, remove punct and numeral and stop words
toks <- tokens(epa_corp, remove_punct = TRUE, remove_numbers = TRUE)

# I added some project-specific stop words here
add_stops <- c(stopwords("en"),"environmental", "justice", "ej", "epa", "public", "comment")
toks1 <- tokens_select(toks, pattern = add_stops, selection = "remove")

```

And now convert to a document-feature matrix

```{r dfm}
# convert to document feature matrix
dfm_comm<- dfm(toks1, tolower = TRUE)
# reduce words to base word
dfm <- dfm_wordstem(dfm_comm)
# remove terms only appearing in one doc (min_termfreq = 10)
dfm <- dfm_trim(dfm, min_docfreq = 2) 

print(head(dfm)) # each comment is a row, each col is a term

# remove rows (docs) with all zeros (these 0's are present bc we removed stop words)
sel_idx <- slam::row_sums(dfm) > 0 
dfm <- dfm[sel_idx, ]
#comments_df <- dfm[sel_idx, ]
```

We somehow have to come up with a value for k,the number of latent topics present in the data. How do we do this? There are multiple methods. Let's use what we already know about the data to inform a prediction. The EPA has 9 priority areas: Rulemaking, Permitting, Compliance and Enforcement, Science, States and Local Governments, Federal Agencies, Community-based Work, Tribes and Indigenous People, National Measures. Maybe the comments correspond to those areas?

```{r LDA_modeling}
k <- 9 

# feed in the DFM and the num of topics to look for and number of iterations, this function estimates the 2 matrices
topicModel_k9 <- LDA(dfm, k, method="Gibbs", control=list(iter = 500, verbose = 25))
#nTerms(dfm_comm) 

tmResult <- posterior(topicModel_k9)
#tmResult

attributes(tmResult)
#ncol(tmResult) # does not run

#nTerms(dfm_comm)   
beta <- tmResult$terms   # get beta from results
dim(beta)                # K distributions over nTerms(DTM) terms# lengthOfVocab
terms(topicModel_k9, 10)
```

Some of those topics seem related to the cross-cutting and additional topics identified in the EPA's response to the public comments:

1. Title VI of the Civil Rights Act of 1964

2.[EJSCREEN](https://www.epa.gov/ejscreen/download-ejscreen-data)

3. climate change, climate adaptation and promoting greenhouse gas reductions co-benefits

4. overburdened communities and other stakeholders to meaningfully, effectively, and transparently participate in aspects of EJ 2020, as well as other agency processes

5. utilize multiple Federal Advisory Committees to better obtain outside environmental justice perspectives

6. environmental justice and area-specific training to EPA staff

7. air quality issues in overburdened communities

So we could guess that there might be a 16 topics (9 priority + 7 additional). Or we could calculate some metrics from the data. (what initial value of k gives us the best model)

```{r LDA_again}
# fit the model by running a series of models, starting with 2 topics and ranging to 20 topics
result <- FindTopicsNumber(
  dfm,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)

FindTopicsNumber_plot(result)
# interpretation:
# top line: the lower the y-axis number the better, so the more topics you add, the better
# bottom line: the higher the number the better, so 7 and 14 looks good
# y-axis has no units 

k <- 7

topicModel_k7 <- LDA(dfm, k, method="Gibbs", control=list(iter = 500, verbose = 25))

tmResult <- posterior(topicModel_k7)
terms(topicModel_k7, 10)
theta <- tmResult$topics
beta <- tmResult$terms
vocab <- (colnames(beta))
```

There are multiple proposed methods for how to measure the best k value. You can [go down the rabbit hole here](#multiple%20proposed%20methods%20for%20measuring%20the%20best%20k%20value.%20You%20can)

```{r top_terms_topic}
comment_topics <- tidy(topicModel_k7, matrix = "beta")

top_terms <- comment_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
# beta is the probability of that term in that topic
```

```{r plot_top_terms}

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

Let's assign names to the topics so we know what we are working with. We can name them by their top terms

```{r topic_names}
top5termsPerTopic <- terms(topicModel_k7, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
# guess the names for the topics
```

We can explore the theta matrix, which contains the distribution of each topic over each document

```{r topic_dists}
exampleIds <- c(1, 2, 3)
N <- length(exampleIds)

#lapply(epa_corp[exampleIds], as.character) #uncomment to view example text
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document=factor(1:N)), variable.name = "topic", id.vars = "document")  
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)
```

Here's a neat JSON-based model visualizer

```{r LDAvis}
library(LDAvis)
library("tsne")
svd_tsne <- function(x) tsne(svd(x)$u)
json <- createJSON(
  phi = tmResult$terms, 
  theta = tmResult$topics, 
  doc.length = rowSums(dfm), 
  vocab = colnames(dfm), 
  term.frequency = colSums(dfm),
  mds.method = svd_tsne,
  plot.opts = list(xlab="", ylab="")
)
serVis(json)
# relevance metric llamda = weighting things highly if they are highly focused, or can choose to weight topics more heavily if they are dispersed broadly thruout doc
```


### Assignment:

Either:

A) continue on with the analysis we started (choose diff values for k and justify the decision for that k-value):

Run three more models and select the overall best value for k (the number of topics) - include some justification for your selection: theory, FindTopicsNumber() optimization metrics, interpretability, LDAvis

### Assignment Model #1

For my first experimental value of `k`, I will test out `k` = 3 because each of the EPA's priority areas seem to fall into one of three main categories: rules and regulations, science and moniroing, and culture and humanities. I would expect to see the most words in the rules and regulations category, because the EPA is focused on that the most, while I expect the least to fall into the culture and humanities section. I think that manually choosing a value of `k` gives me a better understanding of the workflow without having a function choose the best value of `k` right off the bat. This value of `k` is small relative to the values we chose in class. Next, I will use a much larger `k` and compare results. 

```{r}
k <- 3

topicModel_k3 <- LDA(dfm, k, method="Gibbs", control=list(iter = 500, verbose = 25))

tmResult <- posterior(topicModel_k3)

attributes(tmResult)

beta <- tmResult$terms   # get beta from results
dim(beta)                # K distributions over nTerms(DTM) terms# lengthOfVocab
terms(topicModel_k3, 30)
```

### Assigment Model #2

For my first experimental value of `k`, I will test out a much larger number; `k` = 15 because using `k` = 3 did not show much distinction in each category (I saw significant overlap of words). I think that since the EPA's documents seem to include a broader range of topics than just 3, perhaps I should try to create much smaller topics that have a clear focus. The results show that there is more distinction between categories, such as `Topic 11` that seems to include more industry and population-wide industrial issues, compared to `Topic 12` that is more nature-focused. 

```{r}
k <- 15

# feed in the DFM and the num of topics to look for and number of iterations, this function estimates the 2 matrices
topicModel_k15 <- LDA(dfm, k, method="Gibbs", control=list(iter = 500, verbose = 25))

tmResult <- posterior(topicModel_k15)

attributes(tmResult)

beta <- tmResult$terms   # get beta from results
dim(beta)                # K distributions over nTerms(DTM) terms# lengthOfVocab
terms(topicModel_k15, 10)
```

### Assignment Model #3

For the final model for this data, I will use the function `FindTopicsNumber()` to help me visualize the best value of `k`, but I will change it from the class code by adjusting the `topics` argument. 

```{r}
# result <- FindTopicsNumber(
#   dfm,
#   topics = seq(from = 2, to = 20, by = 1),
#   metrics = c("CaoJuan2009",  "Deveaud2014"),
#   method = "Gibbs",
#   control = list(seed = 77),
#   verbose = TRUE
# )

# fit the model by running a series of models, starting with 2 topics and ranging to 20 topics
result <- FindTopicsNumber(
  dfm,
  topics = seq(from = 5, to = 30, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 100),
  verbose = TRUE
)

FindTopicsNumber_plot(result)
```















