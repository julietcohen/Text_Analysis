---
title: "leopard_shark"
author: "Juliet"
date: "4/13/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyr) #text analysis in R
library(lubridate) #working with date data
library(pdftools) #read in pdfs
library(tidyverse)
library(tidytext)
library(here)
library(LexisNexisTools) #Nexis Uni data wrangling
library(sentimentr)
library(readr)
library(textdata)
```

## Read in the data downloaded from Nexis Uni database through the UCSB library

```{r}
leopard_shark_filepath <- list.files(pattern = "leopard_shark_articles.docx", path = getwd(),
                       full.names = TRUE, recursive = TRUE, ignore.case = TRUE)

data <- lnt_read(leopard_shark_filepath)
```

## Use the full text for analysis

```{r}
meta_df <- data@meta
articles_df <- data@articles
paragraphs_df <- data@paragraphs

data2 <- data_frame(element_id = seq(1:length(meta_df$Headline)), Date = meta_df$Date, Headline = meta_df$Headline)

paragraphs_data <- data_frame(element_id = paragraphs_df$Art_ID, Text  = paragraphs_df$Paragraph)

# join the headlines with the paragraphs to access the full text
data3 <- inner_join(data2, paragraphs_data, by = "element_id")
# this df contains columns for the headline of the articles and the paragraphs, with each paragraph as a row, in tidy format
```

## Clean the data and remove polaized words from the sentiment words

```{r}
# make a new column that states TRUE if the row has the string in it
data3_clean <- data3 %>% 
  mutate(http_present = grepl(pattern = "http", x = Text)) 
# nrows = 5250

# check how many rows were TRUE and how many were FALSE
T_F_summary <- data3_clean %>% 
  group_by(http_present) %>% 
  summarize(sum = n()) # 5190 rows should be maintained because they do NOT contain the undesired string

# remove all rows with that string
data3_clean <- data3_clean %>% 
  filter(http_present == FALSE) 

# check that it worked
unique(data3_clean$http_present)

# grab the bing sentiment lexicon from tidytext
nrc_sentiment <- get_sentiments('nrc') 
head(nrc_sentiment, n = 20)

data3_clean_words <- data3_clean  %>%
  select(!http_present) %>% 
  unnest_tokens(output = word, input = Text, token = 'words')

data3_clean_sentiment_words <- data3_clean_words %>% 
  anti_join(stop_words, by = 'word') %>% # remove stop words
  inner_join(nrc_sentiment, by = 'word') %>% # keeps only sentiment words
  filter(!sentiment %in% c("negative", "positive")) # remove the polarized words
```

# Graph results to match Figure 1A from Froelich et al.

```{r}
graph_data <- data3_clean_sentiment_words %>%
  mutate(year = substring(Date, 1, 4)) %>% 
  select(!Date) %>%  # remove Date col because we only care about year, which is a column we retain
  group_by(year, sentiment) %>% # unique years in this df are 2017, 2019, 2020, 2021, 2022, and NA
  summarise(count = n()) %>% 
  drop_na() # drop na rows, we need the year for all observations to make graph

class(graph_data$year) # character
graph_data$year <- as.Date(graph_data$year, format = "%Y")
class(graph_data$year) # Date

ggplot(data = graph_data, aes(x = year, y = count, color = sentiment)) +
  geom_line() +
  labs(y = "Total Count of Sentiment Words",
       title = "Sentiment Word Counts in Leopard Shark Text by Year")
```

# -------------------------------------------------------------------


```{r}
# look at the sentences from the paragraphs
paragraph_sentences <- get_sentences(data3$Text)
class(paragraph_sentences) # list

# add sentiment scores to each paragraph
sentiment_paragraphs <- sentiment(paragraph_sentences)

sentiment_df <- inner_join(data3, sentiment_paragraphs, by = "element_id")

# approximate the polarity of text by grouping variables
sentiment <- sentiment_by(sentiment_df$Text)

sentiment_df_arranged <- sentiment_df %>%
  arrange(sentiment)

# unique(sent_df_sentiment$sentiment)
```

```{r}
sent_df_sentiment$polarity <- ifelse(sent_df$sentiment <0, -1, ifelse(sent_df$sentiment > 0, 1, 0))

custom_stop_words <- bind_rows(tibble(word = c("fishermen"),  
                                      lexicon = c("custom")), 
                               stop_words)
```

```{r}
#bing_sent <- get_sentiments('bing') #grab the bing sentiment lexicon from tidytext
#head(bing_sent, n = 20)
```

```{r}
#unnest to word-level tokens, remove stop words, and join sentiment words

# text_words <- over_text  %>%
#  unnest_tokens(output = word, input = text, token = 'words')
 
# sent_words <- text_words %>% #break text into individual words
#  anti_join(stop_words, by = 'word') %>% #returns only the rows without stop words
#  inner_join(bing_sent, by = 'word') #joins and retains only sentiment words
```

```{r}
# Letâ€™s take a look at the most common sentiment words in the data set
nrc_sent <- get_sentiments('nrc') # requires downloading a large dataset via prompt

# most common words by sentiment
common_sentiment_words <- sent_df_sentiment %>%
  unnest_tokens(output = word, input = text, token = 'words') %>%
  inner_join(nrc_sent) %>%
  count(word, sort = TRUE)

```
























