---
title: "leopard_shark"
author: "Juliet"
date: "4/13/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyr) #text analysis in R
library(lubridate) #working with date data
library(pdftools) #read in pdfs
library(tidyverse)
library(tidytext)
library(here)
library(LexisNexisTools) #Nexis Uni data wrangling
library(sentimentr)
library(readr)
library(textdata)
```

## Read in the data downloaded from Nexis Uni database through the UCSB library

```{r}
leopard_shark_filepath <- list.files(pattern = "leopard_shark_articles.docx", path = getwd(),
                       full.names = TRUE, recursive = TRUE, ignore.case = TRUE)

data <- lnt_read(leopard_shark_filepath)
```

## Use the full text for analysis

```{r}
meta_df <- data@meta
articles_df <- data@articles
paragraphs_df <- data@paragraphs

data2 <- data_frame(element_id = seq(1:length(meta_df$Headline)), Date = meta_df$Date, Headline = meta_df$Headline)

paragraphs_data <- data_frame(element_id = paragraphs_df$Art_ID, Text  = paragraphs_df$Paragraph)

# join the headlines with the paragraphs to access the full text
data3 <- inner_join(data2, paragraphs_data, by = "element_id")
# this df contains columns for the headline of the articles and the paragraphs, with each paragraph as a row, in tidy format
```

## Clean the data and remove polaized words from the sentiment words

```{r}
# make a new column that states TRUE if the row has the string in it
data3_clean <- data3 %>% 
  mutate(http_present = grepl(pattern = "http", x = Text)) 
# nrows = 5250

# check how many rows were TRUE and how many were FALSE
T_F_summary <- data3_clean %>% 
  group_by(http_present) %>% 
  summarize(sum = n()) # 5190 rows should be maintained because they do NOT contain the undesired string

# remove all rows with that string
data3_clean <- data3_clean %>% 
  filter(http_present == FALSE) 

# check that it worked
unique(data3_clean$http_present)

# grab the bing sentiment lexicon from tidytext
nrc_sentiment <- get_sentiments('nrc') 
head(nrc_sentiment, n = 20)

data3_clean_words <- data3_clean  %>%
  select(!http_present) %>% 
  unnest_tokens(output = word, input = Text, token = 'words')

data3_clean_sentiment_words <- data3_clean_words %>% 
  anti_join(stop_words, by = 'word') %>% # remove stop words
  inner_join(nrc_sentiment, by = 'word') %>% # keeps only sentiment words
  filter(!sentiment %in% c("negative", "positive")) # remove the polarized words
```

# Graph annual raw count of each sentiment word to match Figure 1A from Froelich et al.

```{r}
graph_data <- data3_clean_sentiment_words %>%
  mutate(year = substring(Date, 1, 4)) %>% 
  select(!Date) %>%  # remove Date col because we only care about year, which is a column we retain
  group_by(year, sentiment) %>% # unique years in this df are 2017, 2019, 2020, 2021, 2022, and NA
  summarise(count = n()) %>% 
  drop_na() # drop na rows, we need the year for all observations to make graph

max_count <- max(graph_data$count)

class(graph_data$year) # character
graph_data$year <- as.Date(graph_data$year, format = "%Y")
class(graph_data$year) # Date

ggplot(data = graph_data, aes(x = year, y = count, color = sentiment)) +
  geom_line(lwd = 1.5) +
  theme_classic() +
  scale_y_continuous(breaks = seq(0, (max_count +10), by = 250)) +
  theme(legend.position = c(0.2, 0.6),
        legend.background = element_rect(size=0.5, 
                                         linetype="solid",
                                         color = "black")) +
  labs(y = "Total Count of Sentiment Words",
       x = "Year",
       title = "Sentiment Word Counts in Leopard Shark Text by Year, 2017-2022",
       subtitle = "Graph inspiration from Fro") +
  guides(color = guide_legend()) +
  labs(color = "Sentiment")
```

Plot the amount of emotion words (the 8 from nrc) as a percentage of all the emotion words used each day (aggregate text from articles published on the same day). How does the distribution of emotion words change over time? Can you think of any reason this would be the case?

# Graph daily proportions of sentiment words

```{r}
graph_data_prep <- data3_clean_sentiment_words %>%
  drop_na() %>%
  group_by(Date, sentiment) %>%
  summarise(daily_count = n())

daily_total_words <- graph_data_prep %>% 
  group_by(Date) %>% 
  summarise(total_daily_words = sum(daily_count))

graph_data <- left_join(graph_data_prep, daily_total_words, by = "Date")

graph_data_proportions <- graph_data %>% 
  group_by(sentiment, Date) %>% 
  summarise(proportion = (daily_count / total_daily_words))

# ensure Date col is of class Date, a requirement for line graphs
class(graph_data_proportions$Date) # Date
earliest_date <- min(graph_data_proportions$Date)
latest_date <- max(graph_data_proportions$Date)

ggplot(data = graph_data_proportions, aes(x = Date, y = proportion, color = sentiment)) +
  geom_line() +
  theme_classic() +
  theme(legend.position = c(0.7, 0.8),
        legend.background = element_rect(size=0.5, 
                                         linetype="solid",
                                         color = "black")) +
  guides(color = guide_legend(nrow = 2, byrow = TRUE)) +
  scale_x_continuous(breaks = seq(earliest_date, latest_date, by = 365)) +
  labs(y = "Porportion of Sentiment Word Detected",
       x = "Date",
       title = "Proprotion of Sentiment Words Detected by day in 'Leopard Shark' Text, 2017-2022",
       subtitle = "Graph inspiration from Figure 3A from Froelich et al. 2017",
       color = "Sentiment") 
```

























