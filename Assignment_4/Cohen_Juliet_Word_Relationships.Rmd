---
title: "EDS 231 - Word Relationships"
author: "Juliet Cohen"
date: '2022-05-01'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(tidyr) #text analysis in R
library(pdftools)
library(lubridate) #working with date data
library(tidyverse)
library(tidytext)
library(readr)
library(quanteda)
library(readtext) #quanteda subpackage for reading pdf
library(quanteda.textstats)
library(quanteda.textplots)
library(ggplot2)
library(forcats)
library(stringr)
library(quanteda.textplots)
library(widyr)# pairwise correlations
library(igraph) #network plots
library(ggraph)
```

## Import EPA EJ Data

```{r pdf_import}
setwd(".")
files <- list.files(pattern = "*pdf$")

files <- str_subset(files, pattern="EPA")

ej_reports <- lapply(files, pdf_text)

ej_pdf <- readtext(files, docvarsfrom = "filenames", 
                    docvarnames = c("type", "subj", "year"),
                    sep = "_")

#creating an initial corpus containing our data
epa_corp <- corpus(x = ej_pdf, text_field = "text" )
summary(epa_corp)

#I'm adding some additional, context-specific stop words to stop word lexicon
more_stops <-c("2015","2016", "2017", "2018", "2019", "2020", "www.epa.gov", "https")
add_stops<- tibble(word = c(stop_words$word, more_stops)) 
stop_vec <- as_vector(add_stops)
```

Now we'll create some different data objects that will set us up for the subsequent analyses

```{r tidy}
#convert to tidy format and apply my stop words
raw_text <- tidy(epa_corp)

#Distribution of most frequent words across documents
raw_words <- raw_text %>%
  mutate(year = as.factor(year)) %>%
  unnest_tokens(word, text) %>%
  anti_join(add_stops, by = 'word') %>%
  count(year, word, sort = TRUE)

#number of total words by document  
total_words <- raw_words %>% 
  group_by(year) %>% 
  summarize(total = sum(n))

report_words <- left_join(raw_words, total_words)

# for the analysis that we want to do at the word level:
par_tokens <- unnest_tokens(raw_text, output = paragraphs, input = text, token = "paragraphs")

par_tokens <- par_tokens %>%
 mutate(par_id = 1:n())

par_words <- unnest_tokens(par_tokens, output = word, input = paragraphs, token = "words")
```

```{r quanteda_init}
tokens <- tokens(epa_corp, remove_punct = TRUE) # create token obj
toks1<- tokens_select(tokens, min_nchar = 3)
toks1 <- tokens_tolower(toks1)
toks1 <- tokens_remove(toks1, pattern = (stop_vec)) # remove stop words
dfm <- dfm(toks1) # has docs in 1 col, the rows refer to num of occurrences for each word in the corpus in the doc
# fundamental obj for text analysis in quanteda

#first the basic frequency stat
tstat_freq <- textstat_frequency(dfm, n = 5, groups = year)
head(tstat_freq, 10)
```

## 1. What are the most frequent trigrams in the dataset? How does this compare to the most frequent bigrams? Which n-gram seems more informative here, and why?

Start by looking at bigrams:

```{r}
toks2 <- tokens_ngrams(toks1, n=2) # bigram, tokenize, it goes thru the text with a 2 word window and creates every pair with a 1 step, then chooses the highest frequency pairs for this table
dfm2 <- dfm(toks2)
dfm2 <- dfm_remove(dfm2, pattern = c(stop_vec))
freq_words2 <- textstat_frequency(dfm2, n=20)
freq_words2$token <- rep("bigram", 20)
#tokens1 <- tokens_select(tokens1,pattern = stopwords("en"), selection = "remove")

head(freq_words2)
```

The top 5 most frequent bigrams are:\
1. environmental_justice\
2. technical_assistance\
3. drinking_water\
4. public_health\
5. progress_report


```{r convert_dfm}
toks2 <- tokens_ngrams(toks1, n = 3) # trigram, tokenize, it goes thru the text with a 3 word window and creates every pair with a 1 step, then chooses the highest frequency pairs for this table
dfm2 <- dfm(toks2)
dfm2 <- dfm_remove(dfm2, pattern = c(stop_vec))
freq_words2 <- textstat_frequency(dfm2, n=20)
freq_words2$token <- rep("trigram", 20)

head(freq_words2)
```

The top 5 most frequent trigrams are:\
1. justice_fy2017_progress\
2. fy2017_progress_report\
3. environmental_public_health\
4. environmental_justice_fy2017\
5. national_environmental_justice

The trigrams show more repetitive words such as justice, progress, fy2017, and environmental, and appear to be words that do not form a sensical, stand-alone phrase when read together, while the bigrams are more diverse and the words make sense when read together in sequence. Therefore I think that bigrams are more informative here.

## 2. Choose a new focal term to replace "justice" and recreate the correlation table and network (see corr_paragraphs and corr_network chunks). Explore some of the plotting parameters in the cor_network chunk to see if you can improve the clarity or amount of information your plot conveys. Make sure to use a different color for the ties!

I replaces the term "justice" with the term "wildlife" and recreated the correlation table and network. I explored some of the plotting parameters to improve the clarity and amount of information conveyed by the plot. I used a different color for the ties. 

```{r corr_paragraphs}
word_cors <- par_words %>% 
  add_count(par_id) %>% 
  filter(n >= 50) %>% 
  select(-n) %>%
  pairwise_cor(word, par_id, sort = TRUE) # generates correlation coefficients rather than just the number of occurrences, takes a long time to run since there are tons of word pairs
# cols = item1 and item2 and correlation

wildlife_cors <- word_cors %>% 
  filter(item1 == "wildlife")

word_cors %>%
  filter(item1 %in% c("environmental", "wildlife", "equity", "income")) %>%
  group_by(item1) %>%
  top_n(6) %>%
  #slice_max(item1, n = 6) %>%
  ungroup() %>%
  mutate(item1 = as.factor(item1),
  name = reorder_within(item2, correlation, item1)) %>%
  ggplot(aes(y = name, x = correlation, fill = item1)) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~item1, ncol = 2, scales = "free")+
  scale_y_reordered() +
  labs(y = NULL,
         x = NULL,
         title = "Correlations with key words",
         subtitle = "EPA EJ Reports")

#let's zoom in on just one of our key terms
wildlife_cors <- word_cors %>% 
  filter(item1 == "wildlife") %>%
  mutate(n = 1:n())

# Not surprisingly, the correlation between "environmental" and "justice" is by far the highest, which makes sense given the nature of these reports. How might we visualize these correlations to develop of sense of the context in which justice is discussed here?

wildlife_cors  %>%
  filter(n <= 40) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "coral") +
  geom_node_point(size = 3) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.5, "lines")) +
  theme_void()
```

## 3. Write a function that allows you to conduct a keyness analysis to compare two individual EPA reports (hint: that means target and reference need to both be individual reports). Run the function on 3 pairs of reports, generating 3 keyness plots.

Use all of the frequencies for each word in each document and calculate a chi-square to see which words occur significantly more or less within a particular target document.

Example from class:
```{r}
keyness <- textstat_keyness(dfm, target = 2)
textplot_keyness(keyness)
```

Function:
```{r}
keyness_analysis <- function(index) {
    epa_corp_subset <- c(epa_corp[index], epa_corp[index + 1])
    tokens <- tokens(epa_corp_subset, remove_punct = TRUE) # create token obj
    toks1 <- tokens_select(tokens, min_nchar = 3)
    toks1 <- tokens_tolower(toks1)
    toks1 <- tokens_remove(toks1, pattern = (stop_vec)) # remove stop words
    dfm <- dfm(toks1)
    
    keyness <- textstat_keyness(dfm, target = 1)
    return(textplot_keyness(keyness))
}
```

```{r}
keyness_analysis(index = 5)
```



```{r, echo=FALSE, include=FALSE}
# keyness_analysis <- function(data1, data2, target_index) {
#   keyness1 <- textstat_keyness(data1, target = target_index)
#   keyness2 <- textstat_keyness(data2, target = target_index)
#   plot1 <- textplot_keyness(keyness1)
#   plot2 <- textplot_keyness(keyness2)
#   plot <- plot1 / plot2
#   return(plot)
# }
```

Function to conduct a keyness analysis to compare 2 individuals EPA reports at a time:

```{r}
keyness_analysis <- function(data, report1, report2) {
  doc1 <- seq(ndoc(data)) %in% report1
  doc2 <- seq(ndoc(data)) %in% report2
  keyness <- textstat_keyness(doc1, doc2)
  textplot_keyness(keyness)
}

keyness_analysis(data = dfm, report1 = 1, report2 = 2)
```

Test Function:
```{r}
keyness_analysis(data = dfm, target1 = 1, target2 = 2)
```

```{r}
keyness_analysis(data = dfm, target1 = 3, target2 = 4)
```

```{r}
keyness_analysis(data = dfm, target1 = 5, target2 = 6)
```

## 4. Select a word or multi-word term of interest and identify words related to it using windowing and keyness comparison. To do this you will create to objects: one containing all words occurring within a 10-word window of your term of interest, and the second object containing all other words. Then run a keyness comparison on these objects. Which one is the target, and which the reference? Hint

```{r}

```








